Metadata-Version: 2.1
Name: yolox
Version: 0.1.0
Summary: UNKNOWN
Home-page: UNKNOWN
Author: basedet team
License: UNKNOWN
Platform: UNKNOWN
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.6
License-File: LICENSE

b'# MASORT\r\n\r\n## Abstract\r\nHuman-centric video data and algorithms have been driving advances in Multi-Object Tracking (MOT) community. Most popular MOT methods follow the Detection-Based Tracking (DBT) paradigm and achieve impressive performance.  Simple Online and Realtime Tracking (SORT) serves as a foundational association algorithm is continuously optimized in subsequent researches. However, DBT methods still face 2 challenges. On the one hand, when a target suffers from occlusion for prolonged time, Kalman filter (KF)-based motion model trusts the prior state estimations to perform pseudo-updates, leading to significant motion variance. On the other hand, original association paradigms assume behavioral homogeneity among pedestrians and apply the same features for all items. This uniform treatment hinders the advantages of different features can not be fully utilized. To this end, we propose a Measurement-assisted Adaptive association (MASORT) for tracking multi-pedestrian with behavior discrepancies. MASORT adopts the 3-stage association framework of ByteTrack. To fully exploit the effectiveness of motion and Re-ID features, we introduce a Measurement-assisted Motion Enhancement (M2E) module and an Appearance Enhancement (AE) module. M2E improves KF via measurements instead of estimations and proposes a momentum-consistent motion similarity metric. AE dynamically updates Re-ID embeddings based on confidence scores and emphasizes persons with discriminative appearance. Furthermore, we design an Appearance-Motion Adaptive Association (AMAA) algorithm to accommodate behavior discrepancies among individuals. AMAA selects a  suitable feature based on each person\xe2\x80\x99s motion level. Extensive experiments on 3 MOT benchmarks, including MOT17, MOT20 and DanceTrack, demonstrate that MASORT achieves the optimal trade-off between detection and association performance. On the challenging DanceTrack dataset, where the object appearance is highly similar, our MASORT  attains a remarkable 61.4 HOTA. \r\n<p align="center"><img src="assets/masort.jpg" width="800"/></p> \r\n\r\n## News \r\n\r\n## Tracking performance\r\n### Results on MOT challenge and DanceTrack test set\r\n\r\n## Preliminary\r\n### 1. Installing on your local machine \xe2\x8c\xa8\r\nStep1. Install MASORT\r\n```shell\r\ngit clone  https://github.com/YanJieWen/MASORT.git\r\npip  install -r requirements.txt\r\npython setup.py develop\r\n```\r\nStep2. Install motmetrics\r\n```shell\r\ncd MASORT\r\npip install motmetrics\r\n```\r\n\r\nStep3. Install [TrackEval](https://github.com/JonathonLuiten/TrackEval)\r\n```shell\r\ncd external\r\ngit clone https://github.com/JonathonLuiten/TrackEval.git\r\ncd TrackEval\r\npip install -v -e .\r\n```\r\n\r\nStep3. Install [Torchreid](https://github.com/KaiyangZhou/deep-person-reid)\r\n```shell\r\ncd external\r\ngit clone https://github.com/KaiyangZhou/deep-person-reid.git\r\ncd deep-person-reid\r\npip install -r requirements.txt\r\npython setup.py develop\r\n```\r\n\r\nStep4. Install [Fastreid](https://github.com/JDAI-CV/fast-reid)\r\n```shell\r\ncd external\r\ngit clone https://github.com/JDAI-CV/fast-reid.git\r\n```\r\n\r\n## Data preparation\xf0\x9f\x94\xa5\r\n| MOTChallenge | DanceTrack |\r\n|:-----------------:|:----------------:|\r\n|[![MOT](https://img.shields.io/badge/\xf0\x9f\x98\x88mot-blue)](https://motchallenge.net/)|[![Dance](https://img.shields.io/badge/\xf0\x9f\x98\x88dance-challenge-blue)](https://github.com/DanceTrack/DanceTrack)|\r\n\r\nThen, you need to turn the datasets to COCO format and mix different training data:\r\n\r\n```shell\r\ncd MASORT\r\npython tools/convert_mot17_to_coco.py\r\npython tools/convert_mot20_to_coco.py\r\npython tools/convert_crowdhuman_to_coco.py\r\npython tools/convert_cityperson_to_coco.py\r\npython tools/convert_ethz_to_coco.py\r\n```\r\nIt is worth noting that for the MOT17 test set, we uploaded the ``JSON`` file of [FRCNN](datasets/mot/test-FRCNN.json) to accelerate the evaluation.  \r\n\r\nSubsequently, we perform file management operations as instructed in [tools/mix_xxx](tools), and run the following script:\r\n```shell\r\ncd MASORT\r\npython tools/mix_data_ablation.py\r\npython tools/mix_data_test_mot17.py\r\npython tools/mix_data_test_mot20.py\r\n```\r\n\r\n\r\n## Model ZOO\xe2\xad\x90\r\nYou can download the following detection and Re-ID weight, putting them into [pretrained/masortweight](pretrained/masortweight) and [pretrained/reid-model](pretrained/reid-model) folders,respectively. \r\n| Dataset         | HOTA | MOTA | IDF1 |  Model (Detection)                                                                                                                                                                | Model (Re-ID)\r\n| --------------- | ---- | ---- | ---- |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\r\n| MOT17-half-val  | 68.6 | 75.2 | 80.6 | masort_ablation.pth.tar [[baidu:csuw]](https://pan.baidu.com/s/1wXBDlMEIsJi6zGTxGBdkDg) | osnet_ain_ms_d_c.pth.tar [[baidu:csuh]](https://pan.baidu.com/s/1VaSlnfQGY6Pn_KU8IRYiXg)\r\n| MOT17-test      | 64.8 | 78.9 | 78.9 | masort_mot17.pth.tar [[baidu:csuw]](https://pan.baidu.com/s/1wXBDlMEIsJi6zGTxGBdkDg) | mot17_sbs_S50.pth [[baidu:csuh]](https://pan.baidu.com/s/1VaSlnfQGY6Pn_KU8IRYiXg)\r\n| MOT20-test      | 63.8 | 75.7 | 79.0 | masort_mot20.tar [[baidu:csuw]](https://pan.baidu.com/s/1wXBDlMEIsJi6zGTxGBdkDg) | mo20_sbs_S50.pth [[baidu:csuh]](https://pan.baidu.com/s/1VaSlnfQGY6Pn_KU8IRYiXg)\r\n| DanceTrack-val  | 59.3 | 87.9 | 60.8 | masort_dance.pth.tar [[baidu:csuw]](https://pan.baidu.com/s/1wXBDlMEIsJi6zGTxGBdkDg) | dance_sbs_S50.pth [[baidu:csuh]](https://pan.baidu.com/s/1VaSlnfQGY6Pn_KU8IRYiXg)\r\n| DanceTrack-test  | 61.4 | 90.6 | 63.6 | masort_dance.pth.tar [[baidu:csuw]](https://pan.baidu.com/s/1wXBDlMEIsJi6zGTxGBdkDg) | dance_sbs_S50.pth [[baidu:csuh]](https://pan.baidu.com/s/1VaSlnfQGY6Pn_KU8IRYiXg)\r\n* It is important to place the Re-ID configuration file [mot17-sbs](external/mot17-sbs.yml) into [FastReID](external/fast-reid/configs). \r\n## How to Tracking\xe2\x9a\xa1\r\nScripts are in [runs](runs)\r\n```shell\r\nsh runs/mot17-ablation.sh\r\nsh runs/mot17-test.sh\r\nsh runs/mot20-test.sh\r\nsh runs/dance-val.sh\r\nsh runs/dance-test.sh\r\n```\r\n\r\n## Demo \r\n<img src="assets/demo.gif" width="600"/>\r\n\r\n## Citation\r\n**Constructing...**\r\n\r\n## Acknowledgement\r\nA large part of the code is borrowed from [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX), [TrackEval](https://github.com/JonathonLuiten/TrackEval), [Torchreid](https://github.com/KaiyangZhou/deep-person-reid), [FastReid](https://github.com/JDAI-CV/fast-reid), [Deep-OC-SORT](https://github.com/GerardMaggiolino/Deep-OC-SORT/), [ByteTrack](https://github.com/FoundationVision/ByteTrack) and [TOPIC](https://github.com/holmescao/TOPICTrack). Many thanks for their excellent works.\r\n'

